{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "base_path = \"data\"\n",
    "folders = {\n",
    "    300: os.path.join(base_path, \"300\"),\n",
    "    600: os.path.join(base_path, \"600\"),\n",
    "    1200: os.path.join(base_path, \"1200\"),\n",
    "    2400: os.path.join(base_path, \"2400\"),\n",
    "}\n",
    "\n",
    "\n",
    "all_files = {size: glob(f\"{folders[size]}/*.png\") for size in folders}\n",
    "prefixes = set(os.path.basename(f).split(\"_\")[0] for f in all_files[300])\n",
    "\n",
    "paired_images = []\n",
    "for prefix in prefixes:\n",
    "    matched_files = {size: os.path.join(folders[size], f\"{prefix}_GOES16-ABI-ne-GEOCOLOR-{size}x{size}.png\") for size in folders}\n",
    "    \n",
    "    # Ensure all resolutions exist\n",
    "    if all(os.path.exists(matched_files[size]) for size in folders):\n",
    "        paired_images.append(matched_files)\n",
    "\n",
    "# Step 3: Define PyTorch Dataset\n",
    "class SuperResDataset(Dataset):\n",
    "    def __init__(self, paired_images, low_res_size, high_res_size, transform=None):\n",
    "        self.paired_images = paired_images\n",
    "        self.low_res_size = low_res_size\n",
    "        self.high_res_size = high_res_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paired_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_paths = self.paired_images[idx]\n",
    "        \n",
    "        low_res_path = img_paths[self.low_res_size]\n",
    "        high_res_path = img_paths[self.high_res_size]\n",
    "        \n",
    "        low_res = Image.open(low_res_path).convert(\"RGB\")\n",
    "        high_res = Image.open(high_res_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            low_res = self.transform(low_res)\n",
    "            high_res = self.transform(high_res)\n",
    "\n",
    "        return low_res, high_res\n",
    "train_dataset = SuperResDataset(paired_images, low_res_size=300, high_res_size=600)\n",
    "\n",
    "# Example: Load into DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickdsullivan/Projects/SatSR/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent shape: torch.Size([1, 4, 37, 37])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKL\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(\"mps\")\n",
    "\n",
    "\n",
    "dummy_image = Image.new(\"RGB\", (300, 300), color=\"white\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "])\n",
    "image_tensor = transform(dummy_image).unsqueeze(0).to(\"mps\")\n",
    "\n",
    "# Encode and check the shape\n",
    "with torch.no_grad():\n",
    "    latent = vae.encode(image_tensor).latent_dist.sample()\n",
    "\n",
    "print(\"Latent shape:\", latent.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickdsullivan/Projects/SatSR/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 16 files:  19%|█▉        | 3/16 [01:05<04:43, 21.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel ,DiffusionPipeline\n",
    "\n",
    "# Autoencoder\n",
    "vae = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "\n",
    "#Diffusion Model\n",
    "model = UNet2DConditionModel(\n",
    "    sample_size=32,  # Latent space size\n",
    "    in_channels=4,  # Latent space channels (not RGB)\n",
    "    out_channels=4,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 256, 512),\n",
    "    down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\"),\n",
    "    up_block_types=(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# When running in collab\n",
    "#model.to(\"cuda\") \n",
    "#vae.to(\"cuda\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
